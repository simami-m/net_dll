{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание**\n",
        "\n",
        "Постройте модель на основе полносвязных слоёв для классификации Fashion MNIST из библиотеки torchvision (datasets).\n",
        "\n",
        "Получите качество на тестовой выборке не ниже 88%"
      ],
      "metadata": {
        "id": "PWdll99eZ_BI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t8Ma_Abbbgp"
      },
      "outputs": [],
      "source": [
        "import torchvision as tv\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_mnist_train = tv.datasets.FashionMNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
        "f_mnist_test  = tv.datasets.FashionMNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)"
      ],
      "metadata": {
        "id": "NXzx3gGkbpjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_mnist_train[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DenwLICvbwu5",
        "outputId": "3f8d8364-2e15-4ee8-c1fd-38ca6ce8c0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0039, 0.0039, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "          0.0157, 0.0000, 0.0000, 0.0118],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0471, 0.0392, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "          0.3020, 0.5098, 0.2824, 0.0588],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "          0.5529, 0.3451, 0.6745, 0.2588],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "          0.4824, 0.7686, 0.8980, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "          0.8745, 0.9608, 0.6784, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "          0.8627, 0.9529, 0.7922, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "          0.8863, 0.7725, 0.8196, 0.2039],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "          0.9608, 0.4667, 0.6549, 0.2196],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "          0.8510, 0.8196, 0.3608, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "          0.8549, 1.0000, 0.3020, 0.0000],\n",
              "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "          0.8784, 0.9569, 0.6235, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "          0.9137, 0.9333, 0.8431, 0.0000],\n",
              "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "          0.8627, 0.9098, 0.9647, 0.0000],\n",
              "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "          0.8706, 0.8941, 0.8824, 0.0000],\n",
              "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "          0.8745, 0.8784, 0.8980, 0.1137],\n",
              "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "          0.8627, 0.8667, 0.9020, 0.2627],\n",
              "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "          0.7098, 0.8039, 0.8078, 0.4510],\n",
              "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "          0.6549, 0.6941, 0.8235, 0.3608],\n",
              "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "          0.7529, 0.8471, 0.6667, 0.0000],\n",
              "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "          0.3882, 0.2275, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = torch.utils.data.DataLoader(f_mnist_train, 6000, shuffle=True), torch.utils.data.DataLoader(f_mnist_test, 1000, shuffle=True)"
      ],
      "metadata": {
        "id": "CcTzaSvNb8l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train:\n",
        "    print(X.shape, y.shape)\n",
        "    break\n",
        "\n",
        "for X, y in test:\n",
        "    print(X.shape, y.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbkWTf8ucCqR",
        "outputId": "d2d653f6-f73b-49df-f435-486f240508b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6000, 1, 28, 28]) torch.Size([6000])\n",
            "torch.Size([1000, 1, 28, 28]) torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.01\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(784, 512),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(512, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "trainer = torch.optim.Adam(model.parameters(), LR)"
      ],
      "metadata": {
        "id": "ygcWbAaR4FPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "\n",
        "    start=time.time()\n",
        "    train_loss, train_acc, train_n = 0., 0., 0\n",
        "    test_loss, test_acc, test_n = 0., 0., 0\n",
        "    \n",
        "\n",
        "    model.train()\n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_n += len(X)\n",
        "    \n",
        "    model.eval()\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_n += len(X)\n",
        "        \n",
        "    print(\"\\nepoch: {}, taked: {:.2f}, train_loss: {:.3f}, train_acc: {:.2f}%, test_loss: {:.3f}, test_acc: {:.2f}%\".format(\n",
        "        epoch, time.time() - start, train_loss / train_n, train_acc / train_n *100 , test_loss / test_n, test_acc / test_n *100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pje3w8m9cIN7",
        "outputId": "24e15d17-dabe-4cfe-a257-e46007df12bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 1, taked: 13.53, train_loss: 1.957, train_acc: 28.66%, test_loss: 1.125, test_acc: 55.23%\n",
            "\n",
            "epoch: 2, taked: 10.03, train_loss: 0.915, train_acc: 63.03%, test_loss: 0.736, test_acc: 70.80%\n",
            "\n",
            "epoch: 3, taked: 10.02, train_loss: 0.653, train_acc: 75.20%, test_loss: 0.594, test_acc: 77.53%\n",
            "\n",
            "epoch: 4, taked: 10.16, train_loss: 0.528, train_acc: 80.32%, test_loss: 0.517, test_acc: 80.89%\n",
            "\n",
            "epoch: 5, taked: 10.09, train_loss: 0.468, train_acc: 82.91%, test_loss: 0.457, test_acc: 83.39%\n",
            "\n",
            "epoch: 6, taked: 9.98, train_loss: 0.415, train_acc: 85.09%, test_loss: 0.433, test_acc: 84.15%\n",
            "\n",
            "epoch: 7, taked: 10.42, train_loss: 0.382, train_acc: 86.06%, test_loss: 0.407, test_acc: 85.16%\n",
            "\n",
            "epoch: 8, taked: 10.16, train_loss: 0.379, train_acc: 86.01%, test_loss: 0.415, test_acc: 84.59%\n",
            "\n",
            "epoch: 9, taked: 10.93, train_loss: 0.357, train_acc: 86.95%, test_loss: 0.392, test_acc: 85.96%\n",
            "\n",
            "epoch: 10, taked: 10.09, train_loss: 0.329, train_acc: 87.99%, test_loss: 0.381, test_acc: 86.55%\n",
            "\n",
            "epoch: 11, taked: 10.07, train_loss: 0.319, train_acc: 88.30%, test_loss: 0.366, test_acc: 86.80%\n",
            "\n",
            "epoch: 12, taked: 10.02, train_loss: 0.309, train_acc: 88.58%, test_loss: 0.362, test_acc: 87.17%\n",
            "\n",
            "epoch: 13, taked: 13.04, train_loss: 0.302, train_acc: 88.86%, test_loss: 0.376, test_acc: 86.85%\n",
            "\n",
            "epoch: 14, taked: 10.15, train_loss: 0.296, train_acc: 88.97%, test_loss: 0.361, test_acc: 87.28%\n",
            "\n",
            "epoch: 15, taked: 10.02, train_loss: 0.282, train_acc: 89.53%, test_loss: 0.350, test_acc: 87.57%\n",
            "\n",
            "epoch: 16, taked: 10.06, train_loss: 0.273, train_acc: 89.92%, test_loss: 0.345, test_acc: 87.74%\n",
            "\n",
            "epoch: 17, taked: 10.07, train_loss: 0.267, train_acc: 90.04%, test_loss: 0.350, test_acc: 87.50%\n",
            "\n",
            "epoch: 18, taked: 10.01, train_loss: 0.260, train_acc: 90.24%, test_loss: 0.337, test_acc: 87.83%\n",
            "\n",
            "epoch: 19, taked: 9.92, train_loss: 0.252, train_acc: 90.56%, test_loss: 0.332, test_acc: 88.21%\n",
            "\n",
            "epoch: 20, taked: 10.04, train_loss: 0.241, train_acc: 91.05%, test_loss: 0.337, test_acc: 88.40%\n"
          ]
        }
      ]
    }
  ]
}